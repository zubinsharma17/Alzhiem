{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPubI/mWBBeh5GQjBvK/Wia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zubinsharma17/Alzhiem/blob/main/Alzhiem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This error is because of the un-initialized camera."
      ],
      "metadata": {
        "id": "wnqX5iM8ok-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the face detection model\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Initialize the camera\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Initialize the dataset\n",
        "face_data = []\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    # Capture a frame from the camera\n",
        "    ret, frame = cap.read()\n",
        "    \n",
        "    # Convert the frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect the faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    # Draw the bounding box around the detected faces and save the face data\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # Extract the face region of interest (ROI)\n",
        "        offset = 10\n",
        "        face_section = frame[y-offset:y+h+offset, x-offset:x+w+offset]\n",
        "        face_section = cv2.resize(face_section, (100, 100))\n",
        "\n",
        "        # Save the face ROI in the dataset\n",
        "        face_data.append(face_section)\n",
        "        count += 1\n",
        "\n",
        "    # Display the output frame\n",
        "    cv2.imshow('Face Detection', frame)\n",
        "\n",
        "    # Press 'q' to quit the program and save the dataset\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "# Convert the face dataset to a numpy array\n",
        "face_data = np.asarray(face_data)\n",
        "face_data = face_data.reshape((face_data.shape[0], -1))\n",
        "print(face_data.shape)\n",
        "\n",
        "# Save the face dataset as a numpy array\n",
        "np.save('face_data.npy', face_data)\n",
        "print(\"Dataset saved successfully...\")\n",
        "\n",
        "# Release the camera and close all windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "NssBs4e7oUEv",
        "outputId": "f01fa793-2ab1-4de8-c266-adcaa2b94686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c74aa5ddbaeb>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Convert the frame to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Detect the faces in the frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The upper code will save a file named \"face_data.npy\" in the numpy format.\n",
        "this will then be loaaded in the following code."
      ],
      "metadata": {
        "id": "EerekX7kowie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctxgwj7Tn4Ry"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the face dataset\n",
        "face_data = np.load('face_data.npy')\n",
        "\n",
        "# Load the labels for each face in the dataset\n",
        "face_labels = np.arange(face_data.shape[0])\n",
        "face_labels = face_labels.reshape((face_labels.shape[0], 1))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(face_data, face_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a linear regression model on the training set\n",
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "score = reg.score(X_test, y_test)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "# Initialize the camera\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Load the face detection model\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "while True:\n",
        "    # Capture a frame from the camera\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Convert the frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect the faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    # Recognize the faces in the frame using the trained model\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract the face region of interest (ROI)\n",
        "        offset = 10\n",
        "        face_section = frame[y-offset:y+h+offset, x-offset:x+w+offset]\n",
        "        face_section = cv2.resize(face_section, (100, 100))\n",
        "\n",
        "        # Convert the face ROI to a 1D array\n",
        "        face_section = face_section.reshape((1, -1))\n",
        "\n",
        "        # Predict the label for the face using the trained model\n",
        "        label = int(reg.predict(face_section)[0][0])\n",
        "\n",
        "        # Draw the bounding box around the recognized face\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, str(label), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Display the output frame\n",
        "    cv2.imshow('Face Recognition', frame)\n",
        "\n",
        "    # Press 'q' to quit the program\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera and close all windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To add the names to the face which will be recognized we will import a new file named \"names.npy\". These names will be shown on the face.\n",
        "Note: In this code, the names array should be a 1-dimensional numpy array containing the names of the people associated with the face data in the same order as the data."
      ],
      "metadata": {
        "id": "l-JrTphWpNWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the face detection model\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Initialize the camera\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Initialize the dataset\n",
        "face_data = []\n",
        "count = 0\n",
        "\n",
        "# Load the saved face data\n",
        "face_data = np.load('face_data.npy')\n",
        "\n",
        "# Load the saved names associated with the face data\n",
        "names = np.load('names.npy')\n",
        "\n",
        "# Train the linear regression model\n",
        "model = cv2.face.LBPHFaceRecognizer_create()\n",
        "model.train(face_data, np.array(names))\n",
        "\n",
        "while True:\n",
        "    # Capture a frame from the camera\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Convert the frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect the faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    # Draw the bounding box around the detected faces and predict the names\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        # Extract the face region of interest (ROI)\n",
        "        offset = 10\n",
        "        face_section = gray[y-offset:y+h+offset, x-offset:x+w+offset]\n",
        "        face_section = cv2.resize(face_section, (100, 100))\n",
        "\n",
        "        # Predict the name of the face using the trained model\n",
        "        pred = model.predict(face_section)\n",
        "\n",
        "        # Draw the name of the person on the face\n",
        "        cv2.putText(frame, names[pred[0]], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the output frame\n",
        "    cv2.imshow('Face Recognition', frame)\n",
        "\n",
        "    # Press 'q' to quit the program\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera and close all windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "CAsnSmovpCZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **This code only detects faces from the camera and saves them in a dataset. It does not include face recognition functionality. To recognize a face, you would need to train a machine learning model using the dataset of faces you have collected. Once the model is trained, it can then be used to recognize faces in real-time from the camera and label them with their corresponding names. The labeling of the name can be done by adding text to the output frame using the cv2.putText function."
      ],
      "metadata": {
        "id": "7unI3hwyqJL3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XeeJFMhqpsc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}